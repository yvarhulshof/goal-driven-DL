{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "110032bc",
      "metadata": {
        "id": "110032bc"
      },
      "source": [
        "# Session 6: Goal-Driven Deep Learning\n",
        "\n",
        "## About this tutorial\n",
        "In the lecture you have seen how deep learning can be used to model brain regions and functions. This is motivated by the fact that deep neural networks (DNNs) combine brain inspired computational principles with hitherto unseen effectiveness at solving perceptual and motor tasks. This renders DNNs well suited to uncover the kinds of representations and computations that may underlie complex, high-level functions of biological systems. As such, biologically plausible DNNs can be used as generative models to formulate new hypothesis about brain functionality. Furthermore, DNNs can be used for testing hypotheses in neuroscience in silico by training them on ecologically relevant tasks and subsequently exposing them to stimuli used in neuroscientific experimentation. However, while DNNS are biologically inspired, they are not yet particularly biologically realistic. \n",
        "\n",
        "In this tutorial we will address one shortcoming of DNNs, their activation functions are not particularly biologically plausible. Real neurons use discrete spikes whereas DNN units have continuous activation functions. Given what you learned last week, it should nevertheless be possible to train a DNN with a spiking-neuron activation function. This is what you will do in this tutorial. This tutorial constitutes your third formative assessment and you have time to finish it until 6pm on October 11. A solution will become available after that deadline."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28619bac",
      "metadata": {
        "id": "28619bac"
      },
      "source": [
        "## Spiking Activation Function\n",
        "To make a deep neural network spiking, we will utilize an approach put forth by [Hunsberger & Eliasmith (2015)](https://arxiv.org/abs/1510.08829). These authors used the steady-state firing rate of leaky integrate-and-fire (LIF) neurons as activation functions in their neural networks. The steady state firing rate of a LIF neuron can be derived analytically. We start with a simplified neuron model (reversal potential is set to $0$ & conductance is set to $1$):\n",
        "\n",
        "$$\n",
        "\\tau_m \\dot{V} = -V + I\n",
        "$$\n",
        "\n",
        "where $\\tau_m$ is the membrane time constant and $I$ is a constant input current, the steady-state firing rate is given by\n",
        "\n",
        "$$\n",
        "G \\left( I \\right) = \\begin{cases}\n",
        "        \\left[ \\tau_{ref} - \\tau_m \\ln \\left( 1 - \\frac{V_{thr}}{I} \\right) \\right]^{-1}, & \\text{if } I > V_{thr}\\\\\n",
        "        0 & \\text{otherwise}\n",
        "        \\end{cases}\n",
        "$$\n",
        "\n",
        "where $\\tau_{ref}$ is the refactory period and $V_the$ is the threshold. \n",
        "\n",
        "The LIF steady state firing rate has the particular problem that its derivative approaches infinity as $I$ approaches zero from above. This causes problems when employing backpropagation but can be addressed by slightly adjusting the firing rate equation to smooth it out. The equation above can be re-written like this:\n",
        "\n",
        "$$\n",
        "G \\left( I \\right) = \\left[ \\tau_{ref} + \\tau_m \\ln \\left( 1 + \\frac{V_{thr}}{\\rho \\left( I - V_{thr} \\right)} \\right) \\right]^{-1}\n",
        "$$\n",
        "\n",
        "where $\\rho (x) = \\max(x,0)$. If we replace this hard maximum with a softer maximum $\\rho (x) = \\ln\\left( 1 + e^x  \\right)$, then the LIF neuron loses its hard threshold and the derivative becomes bounded. This will be the LIF-based activation function for our spiking DNN."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcfd54f2",
      "metadata": {
        "id": "dcfd54f2"
      },
      "source": [
        "## Deep Learning with PyTorch\n",
        "\n",
        "Before you can start building and training a spiking DNN, you should first have a look at how to build and train any neural network using machine learning libraries like [PyTorch](https://pytorch.org/) or [TensorFlow](https://www.tensorflow.org/). In the present session, we will use the former. \n",
        "\n",
        "In this section, we run through the API for common tasks in deep learning. \n",
        "\n",
        "### Working with data\n",
        "\n",
        "PyTorch has two primitives to work with data: `torch.utils.data.DataLoader` and `torch.utils.data.Dataset`. `Dataset ` stores the samples and their corresponding labels, and `DataLoader` wraps an iterable around the `Dataset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f908a65d",
      "metadata": {
        "id": "f908a65d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa03ae76",
      "metadata": {
        "id": "fa03ae76"
      },
      "source": [
        "PyTorch offers domain-specific libraries which include datasets. For this tutorial, we will be using the `TorchVision` dataset library.\n",
        "\n",
        "The `torchvision.datasets` module contains `Dataset` objects for many real-world vision data like CIFAR. Here, we use the FashionMNIST dataset. Every TorchVision `Dataset` includes two arguments: `transform` and `target_transform` to modify the samples and labels respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a70ebe8",
      "metadata": {
        "id": "5a70ebe8"
      },
      "outputs": [],
      "source": [
        "# Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9364536",
      "metadata": {
        "id": "d9364536"
      },
      "source": [
        "We pass the `Dataset` as an argument to `DataLoader`. This wraps an iterable over our dataset, and supports automatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of $64$, i.e. each element in the dataloader iterable will return a batch of $64$ features and labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea7acd60",
      "metadata": {
        "id": "ea7acd60"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d7af31e",
      "metadata": {
        "id": "3d7af31e"
      },
      "source": [
        "### Creating Models\n",
        "\n",
        "To define a neural network in PyTorch, we create a class that inherits from `nn.Module`. We define the layers of the network in the `__init__` function and specify how data will pass through the network in the forward function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c4f279c",
      "metadata": {
        "id": "5c4f279c"
      },
      "outputs": [],
      "source": [
        "# Set cpu device for training.\n",
        "device = \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Define model\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.conv_stack1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(7, 7), padding=3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
        "        )\n",
        "        self.conv_stack2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(7, 7),padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
        "        )\n",
        "        self.linear_stack = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(800, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        conv1 = self.conv_stack1(x)\n",
        "        conv2 = self.conv_stack2(conv1)\n",
        "        logits = self.linear_stack(conv2)\n",
        "        return logits\n",
        "\n",
        "model = ConvNet().to(device)\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd2296a5",
      "metadata": {
        "id": "bd2296a5"
      },
      "source": [
        "Feel free to play around with the number of channels, filter sizes etc. Make sure, however, that things fit together. For instance, the number of inputs to the first *fully connected* (`Linear`) layer was set to $800$ because $32$ channels $\\times$ $5$ pixels $\\times$ $5$ pixels $=800$. How did we get to $5 \\times 5$ pixels? Note that we have two steps where we have convolution followed by max pooling. There is a neat equation to find the image dimensions after a convolution or pooling operation:\n",
        "\n",
        "$$\n",
        "\\begin{array}{lr}\n",
        "W_o &= \\lfloor \\frac{W_i - F_W + 2 P_W}{S_W} \\rfloor \\\\\n",
        "H_o &= \\lfloor \\frac{H_i - F_H + 2 P_H}{S_H} \\rfloor\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "where $W$ and $H$ stand for width and height, respectively; the subscripts $i$ and $o$ reflect input and output dimensions, respectively; $F$ is the filter (kernel) size; $P$ is the zero-padding; $S$ is the stride; and $\\lfloor \\cdot \\rfloor$ denotes the `floor` operation (rounding down). In our example, we are dealing with square images (i.e., $W=H$).\n",
        "\n",
        "Let's trace the sizes of our layers through the two convolution stacks. \n",
        "\n",
        "#### Conv Stack 1\n",
        "We start with a $28 \\times 28$ gray-scale image such that the input image dimensions are $28 \\times 28 \\times 1$. Then we apply convolutions with $16$ different square filters, each of size $F=7$, zero-padding $P=3$ and no stride $(S=1)$. This leads to a dimension of the first convolutional layer of $28 \\times 28 \\times 16$.\n",
        "\n",
        "Then, we apply a pooling operation with $F=2$ (`kernel_size`), $P=0$ and $S=2$. This leads to the following dimension of the pooling layer $14 \\times 14 \\times 16$.\n",
        "\n",
        "#### Conv Stack 2\n",
        "Then, we apply another convolution operation to the pooling layer of the first stack. We apply convolutions with $32$ different square filters, each of size $F=7$, zero-padding $P=1$ and no stride $(S=1)$. This leads to a dimension of the first convolutional layer of $10 \\times 10 \\times 32$.\n",
        "\n",
        "Finally, we apply another pooling operation with $F=2$ (`kernel_size`), $P=0$ and $S=2$. This leads to the following dimension of the second pooling layer $5 \\times 5 \\times 32$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9eb4a4f",
      "metadata": {
        "id": "c9eb4a4f"
      },
      "source": [
        "### Optimizing the Model Parameters\n",
        "\n",
        "To train a model, we need a loss function and an optimizer. I use the stochastic gradient descent (`SGD`). This is not optimal (`Adam` is much better) but useful to illustrate how the network gradually improves with training. Feel free to play around with other optimizers after you finished the session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04a888de",
      "metadata": {
        "id": "04a888de"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02d5a245",
      "metadata": {
        "id": "02d5a245"
      },
      "source": [
        "In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and backpropagates the prediction error to adjust the model’s parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dffdbf26",
      "metadata": {
        "id": "dffdbf26"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f58ac2c8",
      "metadata": {
        "id": "f58ac2c8"
      },
      "source": [
        "We also check the model’s performance against the test dataset to ensure it is learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "243561c3",
      "metadata": {
        "id": "243561c3"
      },
      "outputs": [],
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7b622ae",
      "metadata": {
        "id": "d7b622ae"
      },
      "source": [
        "The training process is conducted over several iterations (*epochs*). During each epoch, the model learns parameters to make better predictions. We print the model’s accuracy and loss at each epoch; we’d like to see the accuracy increase and the loss decrease with every epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "757827a7",
      "metadata": {
        "id": "757827a7"
      },
      "outputs": [],
      "source": [
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00f8f29e",
      "metadata": {
        "id": "00f8f29e"
      },
      "source": [
        "### Define Custom Activation Function\n",
        "To eventually make the CNN spiking, you cannot use any of the standard activation functions. Instead, you need to define your own activation function based on the F-I (frequency-current) curve of the leaky integrate and fire neuron. Fortunately, defining a custom activation function for PyTorch is (*almost*) as a simple as defining a Python function. The only additional step is to create a class wrapper from PyTorch `nn.Module` to make sure PyTorch can use the function. Also, make sure to use `torch` (rather than e.g. NumPy) operations in your function definition.\n",
        "\n",
        "As an example, we wil implement the sigmoid linear unit (SiLU):\n",
        "\n",
        "$$\n",
        "f(x) = \\frac{x}{1+e^{-x}} = x \\cdot \\sigma (x)\n",
        "$$\n",
        "\n",
        "where $\\sigma (\\cdot)$ is the sigmoid activation function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db1a5b6f",
      "metadata": {
        "id": "db1a5b6f"
      },
      "outputs": [],
      "source": [
        "# activation function\n",
        "def silu(x):\n",
        "    '''\n",
        "    Applies the Sigmoid Linear Unit (SiLU) function element-wise\n",
        "    '''\n",
        "    return torch.sigmoid(x) * x\n",
        "\n",
        "\n",
        "# class wrapper\n",
        "class SiLU(nn.Module):\n",
        "    '''\n",
        "    Shape:\n",
        "        - Input: (N, *) where * means, any number of additional\n",
        "          dimensions\n",
        "        - Output: (N, *), same shape as the input\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super().__init__() # init the base class\n",
        "\n",
        "    def forward(self, x):\n",
        "        return silu(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fde3642b",
      "metadata": {
        "id": "fde3642b"
      },
      "source": [
        "The code below utilizes this function in a CNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60036385",
      "metadata": {
        "id": "60036385"
      },
      "outputs": [],
      "source": [
        "# Define model\n",
        "class SiLUNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiLUNet, self).__init__()\n",
        "        self.silu = SiLU()\n",
        "        self.conv_stack1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(7, 7), padding=3),\n",
        "            self.silu,\n",
        "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
        "        )\n",
        "        self.conv_stack2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(7, 7),padding=1),\n",
        "            self.silu,\n",
        "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
        "        )\n",
        "        self.linear_stack = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(800, 512),\n",
        "            self.silu,\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        conv1 = self.conv_stack1(x)\n",
        "        conv2 = self.conv_stack2(conv1)\n",
        "        logits = self.linear_stack(conv2)\n",
        "        return logits\n",
        "\n",
        "silu_model = SiLUNet().to(device)\n",
        "print(silu_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95ae10b7",
      "metadata": {
        "id": "95ae10b7"
      },
      "source": [
        "Let's train our newly defined `SiLUNet` on the same classification task as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06c63f70",
      "metadata": {
        "id": "06c63f70"
      },
      "outputs": [],
      "source": [
        "silu_optimizer = torch.optim.SGD(silu_model.parameters(), lr=1e-3)\n",
        "\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, silu_model, loss_fn, silu_optimizer)\n",
        "    test(test_dataloader, silu_model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04426410",
      "metadata": {
        "id": "04426410"
      },
      "source": [
        "## Task 1 - (50 points)\n",
        "Create a custom LIF activation function. Be careful to include all neuron parameters required for the activation function. For numerical stability, it might be helpful to add a small value (e.g. $10^{-20}$) to the $\\rho ( \\cdot )$ function (defined in the section entitled *Spiking Activation Function* above).\n",
        "\n",
        "$$\n",
        "\\left[ \\begin{array}{lr}\n",
        "\\tau_m & = 0.02 \\text{s} \\\\\n",
        "\\tau_{ref} & = 0.004 \\text{s} \\\\\n",
        "V_{thr} & = 1 \\text{mV}\n",
        "\\end{array} \\right]\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ffa28c2",
      "metadata": {
        "id": "2ffa28c2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "36592d86",
      "metadata": {
        "id": "36592d86"
      },
      "source": [
        "## Task 2 - (50 points)\n",
        "Create a convolutional neural network that utilizes the LIF activation function for the **first** convolutional layer and train the network to classify the fashion MNIST data set. \n",
        "\n",
        "Note that the LIF activation function can produce quite large values. This is the reason why we only use it in the first convolutional layer (you can choose either the ReLU or SiLU for the other layers) $\\rightarrow$ gradient descent becomes unstable with too many LIF neurons! Even a single layer with a LIF activation function will render this network harder to train than the ones before, so you likely have to adjust the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "869e6854",
      "metadata": {
        "id": "869e6854"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}